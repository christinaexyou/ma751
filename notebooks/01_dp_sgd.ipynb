{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f163a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script tfds is installed in '/usr4/ugrad/cjxu/.local/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorflow\n",
    "!pip install -q tensorflow-datasets\n",
    "!pip install -q tensorflow-privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4cfee73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 11:32:01.963992: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-16 11:32:01.966911: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-16 11:32:02.017271: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-16 11:32:02.018243: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-16 11:32:05.436032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "#tf.compat.v1.disable_v2_behavior()\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow import keras\n",
    "#from keras import layers\n",
    "import tensorflow_privacy\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbd97ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr4/ugrad/cjxu/Documents/MA751\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = os.path.abspath('../')\n",
    "if not os.path.exists(ROOT_DIR):\n",
    "    os.makedirs(ROOT_DIR)\n",
    "os.chdir(ROOT_DIR)\n",
    "\n",
    "print(ROOT_DIR)\n",
    "\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data')\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "43c0beb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset.\n"
     ]
    }
   ],
   "source": [
    "print('Loading the dataset.')\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32')/255\n",
    "x_test = x_test.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d619fd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(sparse = False)\n",
    "\n",
    "y_train_onehot = encoder.fit_transform(y_train)\n",
    "y_test_onehot = encoder.fit_transform(y_test)\n",
    "print(y_train[0])\n",
    "print(y_train_onehot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "130f894d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR10 dataset...\n"
     ]
    }
   ],
   "source": [
    "# # Load the CIFAR10 dataset\n",
    "# print(\"Loading CIFAR10 dataset...\")\n",
    "# (ds_cifar10_train, ds_cifar10_test), ds_cifar10_info = tfds.load(\n",
    "#         'cifar10',\n",
    "#         split=['train', 'test'],\n",
    "#         data_dir= DATA_DIR,\n",
    "#         shuffle_files=True, # load in random order\n",
    "#         as_supervised=True, # Include labels\n",
    "#         with_info=True, # Include info\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ba55e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_img(image, label):\n",
    "#     \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "#     return tf.cast(image, tf.float32) / 255., label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b0a51d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare cifar10 training dataset\n",
    "# ds_cifar10_train = ds_cifar10_train.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# ds_cifar10_train = ds_cifar10_train.cache()     # Cache data\n",
    "# ds_cifar10_train = ds_cifar10_train.shuffle(ds_cifar10_info.splits['train'].num_examples)\n",
    "# ds_cifar10_train = ds_cifar10_train.batch(64)  # <<<<< To change batch size, you have to change it here\n",
    "# ds_cifar10_train = ds_cifar10_train.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# # Prepare cifar10 test dataset\n",
    "# ds_cifar10_test = ds_cifar10_test.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# ds_cifar10_test = ds_cifar10_test.batch(64)    # <<<<< To change batch size, you have to change it here\n",
    "# #ds_cifar10_test = ds_cifar10_test.cache()\n",
    "# ds_cifar10_test = ds_cifar10_test.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a2e9308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "8a017f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "model = tf.keras.models.Sequential() \n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(32, kernel_size =(3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding='same', activation = \"relu\")) \n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation = \"relu\")) \n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding='same', activation = \"relu\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation = \"relu\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(tf.keras.layers.Dropout(rate=0.4))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(rate=0.5))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bfc68f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is to modify the optimizer for the DP library of the tensorflow because it cannot run on the tensorflow 2.0+. \n",
    "This is the recognition credit for the notebook:\n",
    "https://colab.research.google.com/github/ricardocarvalhods/dpcgan/blob/master/DP_CGAN_MNIST.ipynb#scrollTo=hOBQELw7mo9Z&line=21&uniqifier=1\n",
    "\"\"\"\n",
    "from absl import logging\n",
    "import collections\n",
    "\n",
    "from tensorflow_privacy.privacy.dp_query import gaussian_query\n",
    "\n",
    "def make_optimizer_class(cls):\n",
    "  \"\"\"Constructs a DP optimizer class from an existing one.\"\"\"\n",
    "  parent_code = tf.compat.v1.train.Optimizer.compute_gradients.__code__\n",
    "  child_code = cls.compute_gradients.__code__\n",
    "  GATE_OP = tf.compat.v1.train.Optimizer.GATE_OP  # pylint: disable=invalid-name\n",
    "  if child_code is not parent_code:\n",
    "    logging.warning(\n",
    "        'WARNING: Calling make_optimizer_class() on class %s that overrides '\n",
    "        'method compute_gradients(). Check to ensure that '\n",
    "        'make_optimizer_class() does not interfere with overridden version.',\n",
    "        cls.__name__)\n",
    "\n",
    "  class DPOptimizerClass(cls):\n",
    "    \"\"\"Differentially private subclass of given class cls.\"\"\"\n",
    "\n",
    "    _GlobalState = collections.namedtuple(\n",
    "      '_GlobalState', ['l2_norm_clip', 'stddev'])\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dp_sum_query,\n",
    "        num_microbatches=None,\n",
    "        unroll_microbatches=False,\n",
    "        *args,  # pylint: disable=keyword-arg-before-vararg, g-doc-args\n",
    "        **kwargs):\n",
    "      \"\"\"Initialize the DPOptimizerClass.\n",
    "\n",
    "      Args:\n",
    "        dp_sum_query: DPQuery object, specifying differential privacy\n",
    "          mechanism to use.\n",
    "        num_microbatches: How many microbatches into which the minibatch is\n",
    "          split. If None, will default to the size of the minibatch, and\n",
    "          per-example gradients will be computed.\n",
    "        unroll_microbatches: If true, processes microbatches within a Python\n",
    "          loop instead of a tf.while_loop. Can be used if using a tf.while_loop\n",
    "          raises an exception.\n",
    "      \"\"\"\n",
    "      super(DPOptimizerClass, self).__init__(*args, **kwargs)\n",
    "      self._dp_sum_query = dp_sum_query\n",
    "      self._num_microbatches = num_microbatches\n",
    "      self._global_state = self._dp_sum_query.initial_global_state()\n",
    "      # TODO(b/122613513): Set unroll_microbatches=True to avoid this bug.\n",
    "      # Beware: When num_microbatches is large (>100), enabling this parameter\n",
    "      # may cause an OOM error.\n",
    "      self._unroll_microbatches = unroll_microbatches\n",
    "\n",
    "    def compute_gradients(self,\n",
    "                          loss,\n",
    "                          var_list,\n",
    "                          gate_gradients=GATE_OP,\n",
    "                          aggregation_method=None,\n",
    "                          colocate_gradients_with_ops=False,\n",
    "                          grad_loss=None,\n",
    "                          gradient_tape=None,\n",
    "                          curr_noise_mult=0,\n",
    "                          curr_norm_clip=1):\n",
    "\n",
    "      self._dp_sum_query = gaussian_query.GaussianSumQuery(curr_norm_clip, \n",
    "                                                           curr_norm_clip*curr_noise_mult)\n",
    "      self._global_state = self._dp_sum_query.make_global_state(curr_norm_clip, \n",
    "                                                                curr_norm_clip*curr_noise_mult)\n",
    "      \n",
    "\n",
    "      # TF is running in Eager mode, check we received a vanilla tape.\n",
    "      if not gradient_tape:\n",
    "        raise ValueError('When in Eager mode, a tape needs to be passed.')\n",
    "\n",
    "      vector_loss = loss()\n",
    "      if self._num_microbatches is None:\n",
    "        self._num_microbatches = tf.shape(input=vector_loss)[0]\n",
    "      sample_state = self._dp_sum_query.initial_sample_state(var_list)\n",
    "      microbatches_losses = tf.reshape(vector_loss, [self._num_microbatches, -1])\n",
    "      sample_params = (self._dp_sum_query.derive_sample_params(self._global_state))\n",
    "\n",
    "      def process_microbatch(i, sample_state):\n",
    "        \"\"\"Process one microbatch (record) with privacy helper.\"\"\"\n",
    "        microbatch_loss = tf.reduce_mean(input_tensor=tf.gather(microbatches_losses, [i]))\n",
    "        grads = gradient_tape.gradient(microbatch_loss, var_list)\n",
    "        sample_state = self._dp_sum_query.accumulate_record(sample_params, sample_state, grads)\n",
    "        return sample_state\n",
    "    \n",
    "      for idx in range(self._num_microbatches):\n",
    "        sample_state = process_microbatch(idx, sample_state)\n",
    "\n",
    "      if curr_noise_mult > 0:\n",
    "        grad_sums, self._global_state = (self._dp_sum_query.get_noised_result(sample_state, self._global_state))\n",
    "      else:\n",
    "        grad_sums = sample_state\n",
    "\n",
    "      def normalize(v):\n",
    "        return v / tf.cast(self._num_microbatches, tf.float32)\n",
    "\n",
    "      final_grads = tf.nest.map_structure(normalize, grad_sums)\n",
    "      grads_and_vars = final_grads#list(zip(final_grads, var_list))\n",
    "    \n",
    "      return grads_and_vars\n",
    "\n",
    "  return DPOptimizerClass\n",
    "\n",
    "\n",
    "def make_gaussian_optimizer_class(cls):\n",
    "  \"\"\"Constructs a DP optimizer with Gaussian averaging of updates.\"\"\"\n",
    "\n",
    "  class DPGaussianOptimizerClass(make_optimizer_class(cls)):\n",
    "    \"\"\"DP subclass of given class cls using Gaussian averaging.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        l2_norm_clip,\n",
    "        noise_multiplier,\n",
    "        num_microbatches=None,\n",
    "        ledger=None,\n",
    "        unroll_microbatches=False,\n",
    "        *args,  # pylint: disable=keyword-arg-before-vararg\n",
    "        **kwargs):\n",
    "      dp_sum_query = gaussian_query.GaussianSumQuery(\n",
    "          l2_norm_clip, l2_norm_clip * noise_multiplier)\n",
    "\n",
    "      if ledger:\n",
    "        dp_sum_query = privacy_ledger.QueryWithLedger(dp_sum_query,\n",
    "                                                      ledger=ledger)\n",
    "\n",
    "      super(DPGaussianOptimizerClass, self).__init__(\n",
    "          dp_sum_query,\n",
    "          num_microbatches,\n",
    "          unroll_microbatches,\n",
    "          *args,\n",
    "          **kwargs)\n",
    "\n",
    "    @property\n",
    "    def ledger(self):\n",
    "      return self._dp_sum_query.ledger\n",
    "\n",
    "  return DPGaussianOptimizerClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "315bcb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "GradientDescentOptimizer = tf.compat.v1.train.GradientDescentOptimizer\n",
    "DPAdamGaussianOptimizer_new = make_gaussian_optimizer_class(tf.compat.v1.train.AdamOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "82008f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "l2_norm_clip = 1.5\n",
    "batch_size = 64\n",
    "num_microbatches=batch_size\n",
    "noise_multiplier = 1.0\n",
    "\n",
    "optimizer = DPAdamGaussianOptimizer_new(\n",
    "    learning_rate=learning_rate,\n",
    "    l2_norm_clip = l2_norm_clip,\n",
    "    num_microbatches=num_microbatches,\n",
    "    noise_multiplier = noise_multiplier)\n",
    "\n",
    "model.compile(optimizer = optimizer, \n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "            metrics=[tf.keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8caed428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_60 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_60 (Bat  (None, 32, 32, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_61 (Conv2D)          (None, 30, 30, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_61 (Bat  (None, 30, 30, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_30 (MaxPoolin  (None, 15, 15, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 15, 15, 32)        0         \n",
      "                                                                 \n",
      " conv2d_62 (Conv2D)          (None, 15, 15, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_62 (Bat  (None, 15, 15, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_63 (Conv2D)          (None, 13, 13, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_63 (Bat  (None, 13, 13, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_31 (MaxPoolin  (None, 6, 6, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 6, 6, 64)          0         \n",
      "                                                                 \n",
      " conv2d_64 (Conv2D)          (None, 6, 6, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_64 (Bat  (None, 6, 6, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_65 (Conv2D)          (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_65 (Bat  (None, 4, 4, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_32 (MaxPoolin  (None, 2, 2, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 128)               65664     \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 355,754\n",
      "Trainable params: 354,858\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "58633a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr4/ugrad/cjxu/.local/lib/python3.10/site-packages/keras/backend.py:5561: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 17s 21ms/step - loss: 1.7834 - categorical_accuracy: 0.3682 - val_loss: 1.3424 - val_categorical_accuracy: 0.5163\n",
      "Epoch 2/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 1.3100 - categorical_accuracy: 0.5378 - val_loss: 1.0520 - val_categorical_accuracy: 0.6256\n",
      "Epoch 3/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 1.0929 - categorical_accuracy: 0.6174 - val_loss: 0.9372 - val_categorical_accuracy: 0.6738\n",
      "Epoch 4/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.9571 - categorical_accuracy: 0.6676 - val_loss: 0.9466 - val_categorical_accuracy: 0.6704\n",
      "Epoch 5/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.8728 - categorical_accuracy: 0.7007 - val_loss: 0.7783 - val_categorical_accuracy: 0.7298\n",
      "Epoch 6/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.8029 - categorical_accuracy: 0.7265 - val_loss: 0.8016 - val_categorical_accuracy: 0.7311\n",
      "Epoch 7/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.7466 - categorical_accuracy: 0.7475 - val_loss: 0.6872 - val_categorical_accuracy: 0.7650\n",
      "Epoch 8/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.7012 - categorical_accuracy: 0.7649 - val_loss: 0.6539 - val_categorical_accuracy: 0.7783\n",
      "Epoch 9/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.6616 - categorical_accuracy: 0.7778 - val_loss: 0.6052 - val_categorical_accuracy: 0.7950\n",
      "Epoch 10/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.6285 - categorical_accuracy: 0.7893 - val_loss: 0.5867 - val_categorical_accuracy: 0.8014\n",
      "Epoch 11/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.6061 - categorical_accuracy: 0.7983 - val_loss: 0.5563 - val_categorical_accuracy: 0.8135\n",
      "Epoch 12/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.5725 - categorical_accuracy: 0.8063 - val_loss: 0.5407 - val_categorical_accuracy: 0.8177\n",
      "Epoch 13/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.5498 - categorical_accuracy: 0.8142 - val_loss: 0.6501 - val_categorical_accuracy: 0.7859\n",
      "Epoch 14/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.5336 - categorical_accuracy: 0.8207 - val_loss: 0.5332 - val_categorical_accuracy: 0.8225\n",
      "Epoch 15/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.5114 - categorical_accuracy: 0.8282 - val_loss: 0.5634 - val_categorical_accuracy: 0.8096\n",
      "Epoch 16/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.4951 - categorical_accuracy: 0.8317 - val_loss: 0.6069 - val_categorical_accuracy: 0.7987\n",
      "Epoch 17/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.4788 - categorical_accuracy: 0.8385 - val_loss: 0.5030 - val_categorical_accuracy: 0.8307\n",
      "Epoch 18/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.4695 - categorical_accuracy: 0.8438 - val_loss: 0.5056 - val_categorical_accuracy: 0.8354\n",
      "Epoch 19/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.4537 - categorical_accuracy: 0.8452 - val_loss: 0.5445 - val_categorical_accuracy: 0.8266\n",
      "Epoch 20/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.4457 - categorical_accuracy: 0.8487 - val_loss: 0.5610 - val_categorical_accuracy: 0.8138\n",
      "Epoch 21/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.4345 - categorical_accuracy: 0.8543 - val_loss: 0.5095 - val_categorical_accuracy: 0.8296\n",
      "Epoch 22/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.4222 - categorical_accuracy: 0.8573 - val_loss: 0.4990 - val_categorical_accuracy: 0.8341\n",
      "Epoch 23/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.4123 - categorical_accuracy: 0.8603 - val_loss: 0.5037 - val_categorical_accuracy: 0.8358\n",
      "Epoch 24/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.4072 - categorical_accuracy: 0.8614 - val_loss: 0.4799 - val_categorical_accuracy: 0.8440\n",
      "Epoch 25/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.3926 - categorical_accuracy: 0.8662 - val_loss: 0.4915 - val_categorical_accuracy: 0.8382\n",
      "Epoch 26/100\n",
      "782/782 [==============================] - 15s 20ms/step - loss: 0.3935 - categorical_accuracy: 0.8675 - val_loss: 0.5665 - val_categorical_accuracy: 0.8208\n",
      "Epoch 27/100\n",
      "782/782 [==============================] - 15s 20ms/step - loss: 0.3762 - categorical_accuracy: 0.8728 - val_loss: 0.5414 - val_categorical_accuracy: 0.8268\n",
      "Epoch 28/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.3740 - categorical_accuracy: 0.8722 - val_loss: 0.4932 - val_categorical_accuracy: 0.8424\n",
      "Epoch 29/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.3676 - categorical_accuracy: 0.8752 - val_loss: 0.5256 - val_categorical_accuracy: 0.8288\n",
      "Epoch 30/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.3640 - categorical_accuracy: 0.8767 - val_loss: 0.4983 - val_categorical_accuracy: 0.8375\n",
      "Epoch 31/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.3506 - categorical_accuracy: 0.8795 - val_loss: 0.4636 - val_categorical_accuracy: 0.8527\n",
      "Epoch 32/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.3440 - categorical_accuracy: 0.8818 - val_loss: 0.5011 - val_categorical_accuracy: 0.8391\n",
      "Epoch 33/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.3409 - categorical_accuracy: 0.8844 - val_loss: 0.4739 - val_categorical_accuracy: 0.8507\n",
      "Epoch 34/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.3367 - categorical_accuracy: 0.8853 - val_loss: 0.4598 - val_categorical_accuracy: 0.8554\n",
      "Epoch 35/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.3327 - categorical_accuracy: 0.8853 - val_loss: 0.4938 - val_categorical_accuracy: 0.8441\n",
      "Epoch 36/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.3243 - categorical_accuracy: 0.8898 - val_loss: 0.4708 - val_categorical_accuracy: 0.8516\n",
      "Epoch 37/100\n",
      "782/782 [==============================] - 15s 20ms/step - loss: 0.3218 - categorical_accuracy: 0.8894 - val_loss: 0.5201 - val_categorical_accuracy: 0.8414\n",
      "Epoch 38/100\n",
      "782/782 [==============================] - 15s 20ms/step - loss: 0.3186 - categorical_accuracy: 0.8910 - val_loss: 0.5023 - val_categorical_accuracy: 0.8437\n",
      "Epoch 39/100\n",
      "782/782 [==============================] - 15s 20ms/step - loss: 0.3097 - categorical_accuracy: 0.8945 - val_loss: 0.4798 - val_categorical_accuracy: 0.8528\n",
      "Epoch 40/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.3079 - categorical_accuracy: 0.8944 - val_loss: 0.4931 - val_categorical_accuracy: 0.8464\n",
      "Epoch 41/100\n",
      "782/782 [==============================] - 15s 20ms/step - loss: 0.3072 - categorical_accuracy: 0.8943 - val_loss: 0.4637 - val_categorical_accuracy: 0.8554\n",
      "Epoch 42/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2989 - categorical_accuracy: 0.8991 - val_loss: 0.5063 - val_categorical_accuracy: 0.8407\n",
      "Epoch 43/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2967 - categorical_accuracy: 0.8987 - val_loss: 0.4563 - val_categorical_accuracy: 0.8595\n",
      "Epoch 44/100\n",
      "782/782 [==============================] - 15s 20ms/step - loss: 0.2930 - categorical_accuracy: 0.8996 - val_loss: 0.4760 - val_categorical_accuracy: 0.8509\n",
      "Epoch 45/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2867 - categorical_accuracy: 0.9012 - val_loss: 0.4560 - val_categorical_accuracy: 0.8591\n",
      "Epoch 46/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2904 - categorical_accuracy: 0.9011 - val_loss: 0.4920 - val_categorical_accuracy: 0.8489\n",
      "Epoch 47/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2815 - categorical_accuracy: 0.9028 - val_loss: 0.4653 - val_categorical_accuracy: 0.8586\n",
      "Epoch 48/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2784 - categorical_accuracy: 0.9051 - val_loss: 0.4886 - val_categorical_accuracy: 0.8504\n",
      "Epoch 49/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2758 - categorical_accuracy: 0.9040 - val_loss: 0.4591 - val_categorical_accuracy: 0.8585\n",
      "Epoch 50/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2724 - categorical_accuracy: 0.9075 - val_loss: 0.4643 - val_categorical_accuracy: 0.8592\n",
      "Epoch 51/100\n",
      "782/782 [==============================] - 15s 20ms/step - loss: 0.2658 - categorical_accuracy: 0.9108 - val_loss: 0.4525 - val_categorical_accuracy: 0.8620\n",
      "Epoch 52/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2657 - categorical_accuracy: 0.9100 - val_loss: 0.4839 - val_categorical_accuracy: 0.8549\n",
      "Epoch 53/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2693 - categorical_accuracy: 0.9081 - val_loss: 0.4727 - val_categorical_accuracy: 0.8572\n",
      "Epoch 54/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2556 - categorical_accuracy: 0.9126 - val_loss: 0.4682 - val_categorical_accuracy: 0.8603\n",
      "Epoch 55/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2503 - categorical_accuracy: 0.9148 - val_loss: 0.5069 - val_categorical_accuracy: 0.8513\n",
      "Epoch 56/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2590 - categorical_accuracy: 0.9123 - val_loss: 0.4835 - val_categorical_accuracy: 0.8576\n",
      "Epoch 57/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2536 - categorical_accuracy: 0.9130 - val_loss: 0.4609 - val_categorical_accuracy: 0.8606\n",
      "Epoch 58/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2502 - categorical_accuracy: 0.9141 - val_loss: 0.4741 - val_categorical_accuracy: 0.8603\n",
      "Epoch 59/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2483 - categorical_accuracy: 0.9152 - val_loss: 0.5145 - val_categorical_accuracy: 0.8491\n",
      "Epoch 60/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2452 - categorical_accuracy: 0.9164 - val_loss: 0.5128 - val_categorical_accuracy: 0.8541\n",
      "Epoch 61/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2395 - categorical_accuracy: 0.9201 - val_loss: 0.4940 - val_categorical_accuracy: 0.8527\n",
      "Epoch 62/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2451 - categorical_accuracy: 0.9162 - val_loss: 0.5350 - val_categorical_accuracy: 0.8432\n",
      "Epoch 63/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2423 - categorical_accuracy: 0.9166 - val_loss: 0.5026 - val_categorical_accuracy: 0.8580\n",
      "Epoch 64/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2386 - categorical_accuracy: 0.9180 - val_loss: 0.4698 - val_categorical_accuracy: 0.8627\n",
      "Epoch 65/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2337 - categorical_accuracy: 0.9207 - val_loss: 0.4965 - val_categorical_accuracy: 0.8556\n",
      "Epoch 66/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2341 - categorical_accuracy: 0.9207 - val_loss: 0.5004 - val_categorical_accuracy: 0.8596\n",
      "Epoch 67/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2330 - categorical_accuracy: 0.9202 - val_loss: 0.4573 - val_categorical_accuracy: 0.8658\n",
      "Epoch 68/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2292 - categorical_accuracy: 0.9216 - val_loss: 0.4605 - val_categorical_accuracy: 0.8625\n",
      "Epoch 69/100\n",
      "782/782 [==============================] - 15s 20ms/step - loss: 0.2264 - categorical_accuracy: 0.9222 - val_loss: 0.4884 - val_categorical_accuracy: 0.8608\n",
      "Epoch 70/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2283 - categorical_accuracy: 0.9231 - val_loss: 0.4784 - val_categorical_accuracy: 0.8588\n",
      "Epoch 71/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2245 - categorical_accuracy: 0.9231 - val_loss: 0.4843 - val_categorical_accuracy: 0.8629\n",
      "Epoch 72/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2231 - categorical_accuracy: 0.9223 - val_loss: 0.4942 - val_categorical_accuracy: 0.8603\n",
      "Epoch 73/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2199 - categorical_accuracy: 0.9246 - val_loss: 0.4770 - val_categorical_accuracy: 0.8658\n",
      "Epoch 74/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2195 - categorical_accuracy: 0.9250 - val_loss: 0.4599 - val_categorical_accuracy: 0.8674\n",
      "Epoch 75/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2180 - categorical_accuracy: 0.9255 - val_loss: 0.4878 - val_categorical_accuracy: 0.8645\n",
      "Epoch 76/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2217 - categorical_accuracy: 0.9239 - val_loss: 0.4858 - val_categorical_accuracy: 0.8644\n",
      "Epoch 77/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2185 - categorical_accuracy: 0.9268 - val_loss: 0.4857 - val_categorical_accuracy: 0.8627\n",
      "Epoch 78/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.2112 - categorical_accuracy: 0.9278 - val_loss: 0.4672 - val_categorical_accuracy: 0.8640\n",
      "Epoch 79/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2131 - categorical_accuracy: 0.9272 - val_loss: 0.5086 - val_categorical_accuracy: 0.8581\n",
      "Epoch 80/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2121 - categorical_accuracy: 0.9279 - val_loss: 0.4783 - val_categorical_accuracy: 0.8633\n",
      "Epoch 81/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2056 - categorical_accuracy: 0.9306 - val_loss: 0.4956 - val_categorical_accuracy: 0.8637\n",
      "Epoch 82/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2119 - categorical_accuracy: 0.9280 - val_loss: 0.4967 - val_categorical_accuracy: 0.8592\n",
      "Epoch 83/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2101 - categorical_accuracy: 0.9280 - val_loss: 0.4725 - val_categorical_accuracy: 0.8629\n",
      "Epoch 84/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2049 - categorical_accuracy: 0.9306 - val_loss: 0.4747 - val_categorical_accuracy: 0.8642\n",
      "Epoch 85/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2026 - categorical_accuracy: 0.9302 - val_loss: 0.4860 - val_categorical_accuracy: 0.8664\n",
      "Epoch 86/100\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2016 - categorical_accuracy: 0.9305 - val_loss: 0.4816 - val_categorical_accuracy: 0.8637\n",
      "Epoch 87/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.2026 - categorical_accuracy: 0.9311 - val_loss: 0.4701 - val_categorical_accuracy: 0.8618\n",
      "Epoch 88/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.2008 - categorical_accuracy: 0.9308 - val_loss: 0.4764 - val_categorical_accuracy: 0.8666\n",
      "Epoch 89/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.2046 - categorical_accuracy: 0.9299 - val_loss: 0.4502 - val_categorical_accuracy: 0.8692\n",
      "Epoch 90/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1999 - categorical_accuracy: 0.9315 - val_loss: 0.4893 - val_categorical_accuracy: 0.8627\n",
      "Epoch 91/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1965 - categorical_accuracy: 0.9325 - val_loss: 0.4752 - val_categorical_accuracy: 0.8667\n",
      "Epoch 92/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1956 - categorical_accuracy: 0.9335 - val_loss: 0.4850 - val_categorical_accuracy: 0.8623\n",
      "Epoch 93/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1890 - categorical_accuracy: 0.9352 - val_loss: 0.4827 - val_categorical_accuracy: 0.8662\n",
      "Epoch 94/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1945 - categorical_accuracy: 0.9333 - val_loss: 0.4987 - val_categorical_accuracy: 0.8593\n",
      "Epoch 95/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1919 - categorical_accuracy: 0.9346 - val_loss: 0.5015 - val_categorical_accuracy: 0.8691\n",
      "Epoch 96/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1937 - categorical_accuracy: 0.9335 - val_loss: 0.4868 - val_categorical_accuracy: 0.8628\n",
      "Epoch 97/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1877 - categorical_accuracy: 0.9346 - val_loss: 0.4906 - val_categorical_accuracy: 0.8652\n",
      "Epoch 98/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1914 - categorical_accuracy: 0.9347 - val_loss: 0.4940 - val_categorical_accuracy: 0.8641\n",
      "Epoch 99/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1904 - categorical_accuracy: 0.9359 - val_loss: 0.4900 - val_categorical_accuracy: 0.8634\n",
      "Epoch 100/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1880 - categorical_accuracy: 0.9363 - val_loss: 0.5147 - val_categorical_accuracy: 0.8628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b43f7a26620>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,\n",
    "          y_train_onehot, \n",
    "          batch_size = 64 , \n",
    "          epochs = 100, \n",
    "          validation_data = (x_test,y_test_onehot))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
